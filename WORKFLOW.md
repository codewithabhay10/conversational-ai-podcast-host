# AI Podcast Buddy â€” Project Workflow & Technology Guide

## Overview

AI Podcast Buddy is a **local-first, voice-powered AI podcast companion** designed for hands-free conversation while driving. It runs entirely on your machine â€” no cloud APIs, no subscriptions. The system crawls trending tech topics, generates podcast-style conversations using a local LLM, and communicates via voice (speech-to-text + text-to-speech). A modern web interface (Next.js PWA) provides an alternative browser-based experience.

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACES                               â”‚
â”‚                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   CLI Voice App  â”‚          â”‚     Next.js Web App (PWA)       â”‚  â”‚
â”‚   â”‚   (main.py)      â”‚          â”‚     localhost:3000              â”‚  â”‚
â”‚   â”‚   Mic â†’ STT      â”‚          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚   â”‚   Speaker â†’ TTS  â”‚          â”‚  â”‚ Chat UI  â”‚ â”‚ Topic Picker â”‚ â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚ Voice In â”‚ â”‚ Browser TTS  â”‚ â”‚  â”‚
â”‚            â”‚                     â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚            â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                              â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                              â”‚
             â”‚ Python calls          REST + WebSocket
             â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            â–¼                              â–¼                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚              BACKEND LAYER                               â”‚        â”‚
â”‚   â”‚                                                          â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚   â”‚  â”‚ Conversation       â”‚    â”‚ FastAPI Server           â”‚  â”‚        â”‚
â”‚   â”‚  â”‚ Engine             â”‚    â”‚ (api_server.py)          â”‚  â”‚        â”‚
â”‚   â”‚  â”‚ (State Machine)    â”‚    â”‚ REST: /api/*             â”‚  â”‚        â”‚
â”‚   â”‚  â”‚ INTROâ†’EXPLAINâ†’     â”‚    â”‚ WS:   /ws/chat           â”‚  â”‚        â”‚
â”‚   â”‚  â”‚ ASKâ†’REACTâ†’EXPAND   â”‚    â”‚ Streaming tokens         â”‚  â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â”‚   â”‚            â”‚                         â”‚                    â”‚        â”‚
â”‚   â”‚            â–¼                         â–¼                    â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚        â”‚
â”‚   â”‚  â”‚           LLM Module (llm/llm.py)                â”‚    â”‚        â”‚
â”‚   â”‚  â”‚  â€¢ Builds message payloads (system + history)    â”‚    â”‚        â”‚
â”‚   â”‚  â”‚  â€¢ HTTP to Ollama API (chat + streaming)         â”‚    â”‚        â”‚
â”‚   â”‚  â”‚  â€¢ Connection pooling, warm-up, speed tuning     â”‚    â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚        â”‚
â”‚   â”‚                         â”‚                                 â”‚        â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚   â”‚  â”‚ Memory      â”‚  â”‚ Research      â”‚  â”‚ Config        â”‚  â”‚        â”‚
â”‚   â”‚  â”‚ (memory.py) â”‚  â”‚ (research.py) â”‚  â”‚ (config.py)   â”‚  â”‚        â”‚
â”‚   â”‚  â”‚ JSON store  â”‚  â”‚ HN + Reddit   â”‚  â”‚ All settings  â”‚  â”‚        â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ HTTP :11434
                                    â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚    Ollama      â”‚
                            â”‚  (llama3 /     â”‚
                            â”‚   mistral)     â”‚
                            â”‚  Local LLM     â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Technology Stack

### Backend (Python)

| Technology | Purpose | Why Chosen |
|---|---|---|
| **Python 3.11** | Core language | Rich ML/AI ecosystem, rapid development |
| **Ollama** | Local LLM inference | Free, no API keys, runs LLaMA/Mistral locally |
| **faster-whisper** | Speech-to-Text | CTranslate2-optimized Whisper â€” fast on CPU |
| **Coqui TTS** | Text-to-Speech (server) | High-quality neural voices, multi-speaker VITS model |
| **FastAPI** | Web API server | Async, WebSocket support, auto-docs, fast |
| **httpx** | Async HTTP client | Streaming support for Ollama token-by-token relay |
| **uvicorn** | ASGI server | Production-grade, WebSocket support |
| **sounddevice** | Microphone recording | Cross-platform audio capture |
| **scipy** | WAV file I/O | Audio processing utilities |
| **requests** | HTTP client (sync) | Ollama communication in CLI mode |
| **praw / feedparser** | Reddit / RSS parsing | Topic research crawling |

### Frontend (TypeScript)

| Technology | Purpose | Why Chosen |
|---|---|---|
| **Next.js 14** | React framework | App Router, SSR, API rewrites, production-ready |
| **React 18** | UI library | Component model, hooks, reactive state |
| **TypeScript** | Type safety | Catch errors at compile time |
| **next-pwa** | PWA support | Service worker generation, offline caching |
| **Web Speech API** | Browser voice input | Native speech recognition, no server needed |
| **SpeechSynthesis API** | Browser TTS | Instant playback, zero latency, no server load |
| **WebSocket** | Real-time streaming | Token-by-token LLM response streaming |

### Infrastructure

| Component | Role |
|---|---|
| **Ollama** (localhost:11434) | LLM inference engine |
| **FastAPI** (localhost:8000) | Backend API + WebSocket server |
| **Next.js** (localhost:3000) | Frontend dev server |
| **JSON files** (data/) | Memory persistence, topic storage |

---

## Detailed Workflow

### 1. Research Phase (Nightly)

```
python research.py
```

**Flow:**
1. **Fetch Hacker News** â€” Queries the Algolia API for top tech stories from the last 10 days
2. **Fetch Reddit** â€” Hits the public JSON API for top posts from r/technology, r/artificial, r/MachineLearning, r/programming
3. **Rank & Deduplicate** â€” Scores each item by `points + comments Ã— 2`, deduplicates by title similarity, picks top 5
4. **Generate Summaries** â€” Sends each topic to Ollama for a 2-3 sentence conversational summary
5. **Save** â€” Writes ranked topics to `data/topics.json`

**Data flow:**
```
Hacker News API â”€â”€â”
                  â”œâ”€â†’ rank_and_extract_topics() â”€â†’ generate_summaries_with_llm() â”€â†’ topics.json
Reddit JSON API â”€â”€â”˜
```

---

### 2. CLI Voice Loop (main.py)

```
python main.py
```

**Startup sequence:**
1. Check Ollama connection + model availability
2. Warm up LLM (send a tiny prompt to load model into GPU/RAM)
3. Load persistent memory from `data/memory.json`
4. Load pre-crawled topics from `data/topics.json`
5. User picks a topic via terminal menu
6. Load faster-whisper STT model + calibrate mic ambient noise
7. Generate intro via LLM â†’ speak it via Coqui TTS

**Voice loop (repeating):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  ğŸ¤ Record mic audio (auto-stop on silence)         â”‚
â”‚         â”‚                                           â”‚
â”‚         â–¼                                           â”‚
â”‚  ğŸ“ Transcribe with faster-whisper                  â”‚
â”‚         â”‚                                           â”‚
â”‚         â”œâ”€â”€â”€ Empty? â†’ Handle silence (fill-in)      â”‚
â”‚         â”œâ”€â”€â”€ Stop word? â†’ Generate farewell â†’ Exit  â”‚
â”‚         â”‚                                           â”‚
â”‚         â–¼                                           â”‚
â”‚  ğŸ§  Advance conversation state machine              â”‚
â”‚  ğŸ“¦ Build message payload (system + history + input)â”‚
â”‚  ğŸ¤– Send to Ollama â†’ get reply                     â”‚
â”‚         â”‚                                           â”‚
â”‚         â–¼                                           â”‚
â”‚  ğŸ”Š Speak reply via Coqui TTS                      â”‚
â”‚  ğŸ’¾ Update conversation history + memory            â”‚
â”‚         â”‚                                           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3. Conversation Engine (State Machine)

The conversation engine prevents dead air and drives engaging dialogue through a deterministic state machine:

```
 INTRO â”€â”€â†’ EXPLAIN â”€â”€â†’ ASK â”€â”€â†’ REACT â”€â”€â†’ EXPAND
                        â–²                    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              (loops)
```

| State | Behavior | Trigger |
|---|---|---|
| **INTRO** | Introduce topic with energy, hook the listener | Topic selected |
| **EXPLAIN** | Clear explanation with analogies | After intro |
| **ASK** | Personal question to the user | After explain, or 2+ silences |
| **REACT** | React to user's answer, build on it | After user speaks |
| **EXPAND** | New angle, counter-argument, fun fact | After reacting |

Each state injects specific behavioral instructions into the system prompt, steering the LLM's personality.

---

### 4. Memory System

Persistent JSON-based memory that survives across sessions:

```json
{
  "topics_discussed": [{"topic": "...", "timestamp": "..."}],
  "user_opinions": [{"topic": "...", "opinion": "...", "timestamp": "..."}],
  "preferences": {},
  "conversation_count": 5,
  "last_session": "2026-02-07T10:30:00"
}
```

**How it works:**
- **Topics** are logged when selected
- **Opinions** are auto-extracted using keyword matching ("I think...", "I love...", "I hate...")
- **Context summary** is injected into the system prompt so the LLM remembers past conversations
- Session counter tracks how many times the user has chatted

---

### 5. Web Interface Workflow (Next.js PWA)

**Startup:**
```
Terminal 1: python api_server.py     â†’ localhost:8000
Terminal 2: cd web && npm run dev    â†’ localhost:3000
```

**User flow:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser opens localhost:3000                   â”‚
â”‚         â”‚                                       â”‚
â”‚         â–¼                                       â”‚
â”‚  ğŸ“¡ Check API health + Ollama status            â”‚
â”‚  ğŸ“‹ Fetch topics from /api/topics               â”‚
â”‚         â”‚                                       â”‚
â”‚         â–¼                                       â”‚
â”‚  ğŸ¯ TOPIC SELECTOR SCREEN                       â”‚
â”‚     â€¢ Browse trending topics (from research)    â”‚
â”‚     â€¢ Enter custom topic                        â”‚
â”‚     â€¢ Refresh topics button                     â”‚
â”‚         â”‚                                       â”‚
â”‚         â–¼ (user picks topic)                    â”‚
â”‚                                                 â”‚
â”‚  ğŸ’¬ CHAT SCREEN                                 â”‚
â”‚     â€¢ WebSocket connects to /ws/chat            â”‚
â”‚     â€¢ Sends "start_topic" message               â”‚
â”‚     â€¢ AI generates intro (streamed token-by-    â”‚
â”‚       token via WebSocket)                      â”‚
â”‚     â€¢ Browser SpeechSynthesis auto-speaks reply  â”‚
â”‚         â”‚                                       â”‚
â”‚         â–¼ (conversation loop)                   â”‚
â”‚                                                 â”‚
â”‚  ğŸ§‘ User types text OR taps mic (Web Speech API)â”‚
â”‚         â”‚                                       â”‚
â”‚         â–¼                                       â”‚
â”‚  ğŸ“¤ Send via WebSocket                          â”‚
â”‚  ğŸ“¥ Receive streamed tokens â†’ render live       â”‚
â”‚  ğŸ”Š Auto-speak completed reply                  â”‚
â”‚  ğŸ’¾ Server updates memory + history             â”‚
â”‚         â”‚                                       â”‚
â”‚         â””â”€â”€â”€â”€ loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**WebSocket message protocol:**

| Direction | Type | Payload |
|---|---|---|
| Client â†’ Server | `start_topic` | `{topic, topicContext}` |
| Client â†’ Server | `message` | `{text, topic, history}` |
| Server â†’ Client | `token` | `{content}` (single token) |
| Server â†’ Client | `complete` | `{content, history, state}` |
| Server â†’ Client | `error` | `{message}` |

---

### 6. TTS Pipeline

Two TTS paths are available:

**Browser TTS (default â€” instant):**
```
AI reply text â†’ SpeechSynthesis API â†’ speaker
               (runs entirely in browser)
               Rate: 1.15x, auto-selects best voice
```

**Server TTS (fallback â€” Coqui):**
```
AI reply text â†’ POST /api/tts â†’ Coqui VITS model â†’ WAV file â†’ HTTP response â†’ Audio element
               (requires TTS model loaded on server)
               Speaker: p273 (energetic male)
```

---

### 7. LLM Communication

All LLM interaction goes through Ollama's local HTTP API:

**Message construction:**
```
[
  { role: "system",    content: SYSTEM_PROMPT + state instructions + memory context },
  { role: "system",    content: topic research context },
  { role: "user",      content: <history msg 1> },
  { role: "assistant", content: <history msg 2> },
  ...last 10 messages...
  { role: "user",      content: <current user input> }
]
```

**Speed tuning parameters:**
| Parameter | Value | Effect |
|---|---|---|
| `num_predict` | 150 | Caps response to ~150 tokens |
| `num_ctx` | 2048 | Smaller context window = faster |
| `temperature` | 0.7 | Balanced creativity |
| `top_k` | 40 | Limits token sampling pool |
| `top_p` | 0.9 | Nucleus sampling threshold |
| `repeat_penalty` | 1.1 | Reduces repetition |

---

## File Structure

```
ai-host/
â”œâ”€â”€ main.py                   # CLI voice loop entry point
â”œâ”€â”€ api_server.py             # FastAPI backend for web UI
â”œâ”€â”€ config.py                 # All configuration in one place
â”œâ”€â”€ conversation_engine.py    # State machine for dialogue flow
â”œâ”€â”€ memory.py                 # JSON-based persistent memory
â”œâ”€â”€ research.py               # Nightly topic crawler (HN + Reddit)
â”œâ”€â”€ generate_icons.py         # PWA icon generator
â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ llm.py                # Ollama communication (chat, stream, warmup)
â”‚
â”œâ”€â”€ stt/
â”‚   â””â”€â”€ stt.py                # Mic recording + faster-whisper transcription
â”‚
â”œâ”€â”€ tts/
â”‚   â””â”€â”€ tts_engine.py         # Coqui TTS synthesis + audio playback
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ memory.json           # Persistent user memory
â”‚   â””â”€â”€ topics.json           # Pre-crawled discussion topics
â”‚
â””â”€â”€ web/                      # Next.js PWA frontend
    â”œâ”€â”€ next.config.js        # PWA config + API proxy rewrites
    â”œâ”€â”€ package.json
    â”œâ”€â”€ tsconfig.json
    â”œâ”€â”€ public/
    â”‚   â”œâ”€â”€ manifest.json     # PWA manifest
    â”‚   â”œâ”€â”€ sw.js             # Service worker
    â”‚   â””â”€â”€ icons/            # App icons (72â€“512px)
    â””â”€â”€ src/
        â”œâ”€â”€ app/
        â”‚   â”œâ”€â”€ layout.tsx    # Root layout + PWA meta tags
        â”‚   â”œâ”€â”€ globals.css   # Dark theme styles
        â”‚   â””â”€â”€ page.tsx      # Main app (topic selector + chat)
        â”œâ”€â”€ components/
        â”‚   â”œâ”€â”€ TopicSelector.tsx   # Topic browsing UI
        â”‚   â”œâ”€â”€ ChatMessages.tsx    # Message bubbles + streaming
        â”‚   â””â”€â”€ ChatInput.tsx       # Text input + mic button
        â”œâ”€â”€ hooks/
        â”‚   â”œâ”€â”€ useWebSocketChat.ts      # WebSocket streaming hook
        â”‚   â”œâ”€â”€ useSpeechRecognition.ts  # Browser voice input hook
        â”‚   â””â”€â”€ useTTS.ts               # Hybrid TTS (browser + server)
        â””â”€â”€ lib/
            â””â”€â”€ api.ts        # REST API client functions
```

---

## Running the Project

```bash
# 1. Start Ollama
ollama serve
ollama pull llama3

# 2. (Optional) Crawl fresh topics
python research.py

# 3a. CLI mode (voice)
python main.py

# 3b. Web mode
python api_server.py          # Terminal 1 â†’ localhost:8000
cd web && npm install && npm run dev   # Terminal 2 â†’ localhost:3000
```

---

## Key Design Decisions

| Decision | Rationale |
|---|---|
| **Local-first (Ollama)** | No API costs, no internet dependency, full privacy |
| **State machine conversation** | Prevents awkward silence, maintains podcast energy |
| **Browser TTS over server TTS** | Zero latency â€” no network round-trip for speech |
| **WebSocket streaming** | Token-by-token display feels responsive even on slow hardware |
| **JSON memory persistence** | Simple, no database dependency, human-readable |
| **PWA** | Installable on phone, works offline for cached content |
| **num_predict cap (150)** | Forces concise replies, reduces generation time |
| **Separated research phase** | Crawling is slow; doing it nightly keeps startup fast |
